#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import subprocess
import os
import sets
import time
import optparse

# solaris doesn't have python 2.5, we copy code from the Python library this as a compatibility measure
try: from subprocess import CalledProcessError
except ImportError:
	class CalledProcessError(Exception):
		"""This exception is raised when a process run by check_call() returns
		a non-zero exit status.  The exit status will be stored in the
		returncode attribute."""
		def __init__(self, returncode, cmd):
			self.returncode = returncode
			self.cmd = cmd
		def __str__(self):
			return "Command '%s' returned non-zero exit status %d" % (self.cmd, self.returncode)


#===================== configuration =====================

parser = optparse.OptionParser("usage: %prog [-np] [-b BUFSIZE]")
parser.add_option('-n', '--dry-run', action='store_true', dest='dryrun', default=False, help='don\'t actually manipulate any file systems')
parser.add_option('-p', '--prefix', action='store', dest='prefix', default="zmirror-", help='prefix to prepend to snapshot names (default: %default)')
parser.add_option('-o', '--progress', action='store_true', dest='progress', default=False, help='show progress (depends on the executabilty of the \'bar\' program) (default: %default)')
parser.add_option('-b', '--bufsize', action='store', dest='bufsize', default=1048576, help='buffer size in bytes for network operations (default: %default)')
opts,args = parser.parse_args(sys.argv[1:])

try:
	bufsize = int(opts.bufsize)
	assert bufsize >= 16384
except (ValueError,AssertionError),e:
	sys.stderr.write("error: bufsize must be an integer greater than 16384\n%s"%parser.get_usage())
	sys.exit(os.EX_USAGE)

if len(args) == 2:
	try: source_host, source_dataset_name = args[0].split(":",1)
	except ValueError: source_host, source_dataset_name = "localhost",args[0]
	try: destination_host, destination_dataset_name = args[1].split(":",1)
	except ValueError: destination_host, destination_dataset_name = "localhost",args[1]
else:
	sys.stderr.write("error: arguments are wrong\n%s"%parser.get_usage())
	sys.exit(os.EX_USAGE)

snapshot_prefix = opts.prefix
snapshot_postfix = lambda: time.strftime("%Y-%m-%d-%H%M")

#===================== end configuration =================


def children_first(pathlist): return sorted(pathlist,key=lambda x:-x.count("/"))
def parents_first(pathlist): return sorted(pathlist,key=lambda x:x.count("/"))
chronosorted = sorted


def run_command(cmd,inp=None,capture_stderr=False):
	if capture_stderr:
		p = subprocess.Popen(cmd,stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
	else:
		p = subprocess.Popen(cmd,stdin=subprocess.PIPE,stdout=subprocess.PIPE)

	if inp:	stdout,stderr = p.communicate(inp)
	else:	stdout,stderr = p.communicate()

	exit = p.wait()
	if exit != 0:
		c = CalledProcessError(exit,cmd)
		raise c
	return stdout,stderr


class Dataset:
	name = None
	children = None
	parent = None
	invalidated = False
	def __init__(self,name,parent=None):
		self.name = name
		self.children = []
		if parent:
			self.parent = parent
			self.parent.add_child(self)

	def add_child(self,child):
		self.children.append(child)
		return child

	def get_child(self,name):
		child = [ c for c in self.children if c.name == name and isinstance(c,Dataset) ]
		assert len(child) < 2
		if not child: raise KeyError,name
		return child[0]

	def get_snapshots(self,flt=True):
		if flt is True: flt = lambda x:True
		children = [ c for c in self.children if isinstance(c,Snapshot) and flt(c) ]
		return children

	def get_snapshot(self,name):
		children = [ c for c in self.get_snapshots() if c.name == name ]
		assert len(children) < 2
		if not children: raise KeyError,name
		return children[0]

	def lookup(self,name): # FINISH THIS
		if "@" in name:
			path,snapshot = name.split("@")
		else:
			path = name
			snapshot = None

		if "/" not in path:
			try: dset = self.get_child(path)
			except KeyError: raise KeyError,"No such dataset %s at %s" %(path,self.get_path())
			if snapshot:
				try: dset = dset.get_snapshot(snapshot)
				except KeyError: raise KeyError,"No such snapshot %s at %s" %(snapshot,dset.get_path())
		else:
			head,tail = path.split("/",1)
			try: child = self.get_child(head)
			except KeyError: raise KeyError,"No such dataset %s at %s" %(head,self.get_path())
			if snapshot: tail = tail + "@" + snapshot
			dset = child.lookup(tail)

		return dset

	def remove(self,child):
		if child not in self.children: raise KeyError, child.name
		child.invalidated = True
		child.parent = None
		self.children.remove(child)
		for c in child.children:
			child.remove(c)

	def get_path(self):
		if not self.parent: return self.name
		return "%s/%s"%(self.parent.get_path(),self.name)

	def get_relative_name(self):
		if not self.parent: return self.name
		return self.get_path()[len(self.parent.get_path())+1:]

	def walk(self):
		if self.invalidated: raise Exception, "%s invalidated"%self
		yield self
		for c in self.children:
			for element in c.walk():
				yield element
			
	def __iter__(self):
		return self.walk()

	def __str__(self):
		return "<Dataset:  %s>"%self.get_path()
	__repr__ = __str__


class Pool(Dataset):
	def __str__(self):
		return "<Pool:     %s>"%self.get_path()
	__repr__ = __str__


class Snapshot(Dataset):
	#def __init__(self,name):
		#Dataset.__init__(self,name)
	def get_path(self):
		if not self.parent: return self.name
		return "%s@%s"%(self.parent.get_path(),self.name)

	def __str__(self):
		return "<Snapshot: %s>"%self.get_path()
	__repr__ = __str__


class PoolSet: # maybe rewrite this as a dataset or something?
	pools = None

	def __init__(self):
		self.pools = {}

	def lookup(self,name):
		if "@" in name:
			path,snapshot = name.split("@")
		else:
			path = name
			snapshot = None

		if "/" not in path:
			try: dset = self.pools[path]
			except KeyError: raise KeyError,"No such pool %s" %(name)
			if snapshot:
				try: dset = dset.get_snapshot(snapshot)
				except KeyError: raise KeyError,"No such snapshot %s at %s" %(snapshot,dset.get_path())
		else:
			head,tail = path.split("/",1)
			try: pool = self.pools[head]
			except KeyError: raise KeyError,"No such pool %s" %(head)
			if snapshot: tail = tail + "@" + snapshot
			dset = pool.lookup(tail)

		return dset

	def parse_zfs_r_output(self,output):

		#print "***Parsing ZFS output***"

		# make into array
		lines = [ s.strip() for s in output.splitlines() ]

		# names of pools
		old_dsets = [ x.get_path() for x in self.walk() ]
		old_dsets.reverse()
		new_dsets = [ s.split("	")[0] for s in lines ]
		
		for dset in new_dsets:
			if "@" in dset: dset, snapshot = dset.split("@")
			else: snapshot = None
			if "/" not in dset: # pool name
				if dset in self.pools: continue
				self.pools[dset] = Pool(dset)
				fs = self.pools[dset]
				#print "	Adding %s"%fs
			else:
				poolname, pathcomponents = dset.split("/")[0],dset.split("/")[1:]
				fs = self.pools[poolname]
				for pcomp in pathcomponents:
					# traverse the child hierarchy or create if that fails
					try: fs = fs.get_child(pcomp)
					except KeyError:
						fs = Dataset(pcomp,fs)
						#print "	Adding %s"%fs

			if snapshot:
				if snapshot not in [ x.name for x in fs.children ]:
					fs = Snapshot(snapshot,fs)
					#print "	Adding %s"%fs

		for dset in old_dsets:
			if dset not in new_dsets:
				#print "	Removing %s"%dset
				if "/" not in dset and "@" not in dset: # a pool
					self.remove(dset)
				else:
					d = self.lookup(dset)
					d.parent.remove(d)

	def remove(self,name): # takes a NAME, unlike the child that is taken in the remove of the dataset method
		for c in self.pools[name].children:
			self.pools[name].remove(c)
		self.pools[name].invalidated = True
		del self.pools[name]

	def __getitem__(self,name):
		return self.pools[name]

	def __str__(self):
		return "<PoolSet at %s>"%id(self)
	__repr__ = __str__

	def walk(self):
		for item in self.pools.values():
			for dset in item.walk():
				yield dset

	def __iter__(self):
		return self.walk()


class ZFSConnection:
	host = None
	_poolset = None
	_dirty = True
	def __init__(self,host="localhost"):
		self.host = host
		self._poolset= PoolSet()
		if host in ['localhost','127.0.0.1']:
			self.command = ["zfs"]
		else:
			self.command = ["ssh","-o","BatchMode yes","-c","arcfour",self.host,"zfs"]

	def _get_poolset(self):
		if self._dirty:
			stdout,stderr = run_command(self.command+["list","-r","-t","all","-H"])
			self._poolset.parse_zfs_r_output(stdout)
			self._dirty = False
		return self._poolset
	pools = property(_get_poolset)

	def create_dataset(self,name):
		run_command(self.command+["create","-o","mountpoint=none",name])
		self._dirty = True
		return self.pools.lookup(name)

	def destroy(self,name):
		run_command(self.command+["destroy",'-r',name])
		self._dirty = True

	def snapshot_recursively(self,name,snapshotname):
		run_command(self.command+["snapshot","-r","%s@%s"%(name,snapshotname)])
		self._dirty = True

	def send(self,name,opts=None):
		if not opts: opts = []
		cmd = self.command + ["send"] + opts + [name]
		p = subprocess.Popen(cmd,stdin=subprocess.PIPE,stdout=subprocess.PIPE,bufsize=bufsize)
		return p

	def receive(self,name,pipe,opts=None):
		if not opts: opts = []
		cmd = self.command + ["receive"] + opts + [name]
		p = subprocess.Popen(cmd,stdin=pipe,stdout=subprocess.PIPE,bufsize=bufsize)
		return p

	def transfer(src_conn,dst_conn,s,d,fromsnapshot=None):
		if fromsnapshot: fromsnapshot=["-i",fromsnapshot]
		else: fromsnapshot = []
		sndprg = src_conn.send(s,opts=["-v"]+fromsnapshot)
		
		if opts.progress:
		    try: barprg = subprocess.Popen(
			["clpbar","-dan","-bs",str(bufsize)],
			stdin=sndprg.stdout,stdout=subprocess.PIPE,bufsize=bufsize)
		    except OSError:
			os.kill(sndprg.pid,15)
			raise
		else:
			barprg = sndprg
		try: rcvprg = dst_conn.receive(d,pipe=barprg.stdout,opts=["-vFu"])
		except OSError:
			os.kill(sndprg.pid,15)
			os.kill(barprg.pid,15)
			raise

		ret = rcvprg.wait()
		if ret:
			os.kill(sndprg.pid,15)
			if opts.progress: os.kill(barprg.pid,15)
		ret2 = sndprg.wait()
		if opts.progress: ret4 = barprg.wait()
		if ret:  raise CalledProcessError(ret,["zfs","receive"])
		if ret2: raise CalledProcessError(ret,["zfs","send"])
		if opts.progress:
			if ret4: raise CalledProcessError(ret,["clpbar"])
		
		dst_conn._dirty = True


# ================ start program algorithm ===================

src_conn = ZFSConnection(source_host)
dst_conn = ZFSConnection(destination_host)
flt = lambda x: x.name.startswith(snapshot_prefix)
snapshot_unique_name = snapshot_prefix + snapshot_postfix()


print "Mirroring dataset %s:%s into %s:%s"%(source_host,source_dataset_name,
	destination_host,destination_dataset_name)

print ""

print "Assessing that the source dataset exists...",
try:
	source_dataset = src_conn.pools.lookup(source_dataset_name)
	print "%s: OK"%source_dataset
except KeyError:
	print "No.\nError: the source dataset does not exist.  Backup cannot continue"
	sys.exit(2)

print ""


try: destination_dataset = dst_conn.pools.lookup(destination_dataset_name)
except KeyError: destination_dataset = None
print "Does the destination dataset exist on the destination side? ",

if destination_dataset:
	print "Yes."
else:
	print "No.\nError: the destination dataset does not exist.  Backup cannot continue"
	sys.exit(2)

print ""



print "Backing up"

print ""


# THE BACKUP ALGORITHM
backed_up_datasets = sets.Set()
for srcpath in [ x.get_path() for x in source_dataset.walk() if not isinstance(x,Snapshot) ]:
	
	src = src_conn.pools.lookup(srcpath)
	dstpath = destination_dataset_name + srcpath[len(source_dataset.get_path()):]

	try: dst = dst_conn.pools.lookup(dstpath)
	except KeyError: dst = None

	print "	Source:	%s"%(srcpath)
	if src.get_snapshots(flt):
		for y in src.get_snapshots(flt): print "		%s"%y.name
	else: print "		***No snapshots***"

	ssn = chronosorted([ x.name for x in src.get_snapshots(flt) ]) # Source Snapshot Names

	print "	Destination: %s"%(dstpath)
	if dst and dst.get_snapshots(flt):
		for y in dst.get_snapshots(flt): print "		%s"%y.name
	elif dst: print "		***No snapshots***"
	else: print "		Dataset does not exist yet"

	if dst: dsn = chronosorted([ x.name for x in dst.get_snapshots(flt) ]) # Destination Snapshot Names
	else:   dsn = []

	no =    lambda x: len(x) == 0
	one =   lambda x: len(x) == 1
	two =   lambda x: len(x) == 2
	first = lambda x: x[0]
	last =  lambda x: x[-1]
	mksn =  lambda x,y: "%s@%s"%(x,y) # make snapshot name

	if no(ssn):
		if no(dsn):
			# no snapshots here or there
			print "	s0r0: snapshotting and sending full snapshot"
			if not opts.dryrun:
				src_conn.snapshot_recursively(srcpath,snapshot_unique_name)
				src_conn.transfer(dst_conn, mksn(srcpath,snapshot_unique_name), mksn(dstpath,snapshot_unique_name))
		else:
			print "	s0r>0: destroying receiver side filesystems, snapshotting and sending full snapshot"
			if not opts.dryrun:
				dst_conn.destroy(dstpath)
				src_conn.snapshot_recursively(srcpath,snapshot_unique_name)
				src_conn.transfer(dst_conn, mksn(srcpath,snapshot_unique_name), mksn(dstpath,snapshot_unique_name))

	elif one(ssn):
		if no(dsn):
			# one snapshot on the sender, none here
			print "	s1r0: sending latest snapshot in full"
			if not opts.dryrun:
				src_conn.transfer(dst_conn, mksn(srcpath,last(ssn)), mksn(dstpath,last(ssn)))
		else:
			if last(ssn) == last(dsn):
				# matching snapshots on both sides
				print "	s1rXmatch: snapshotting and sending incremental snapshot"
				if not opts.dryrun:
					src_conn.snapshot_recursively(srcpath,snapshot_unique_name)
					src_conn.transfer(dst_conn, mksn(srcpath,snapshot_unique_name),	mksn(dstpath,snapshot_unique_name), last(ssn))
			else:
				print "	s1rXnomatch: destroying receiver side filesystems, sending full snapshot"
				if not opts.dryrun:
					dst_conn.destroy(dstpath)
					src_conn.transfer(dst_conn, mksn(srcpath,last(ssn)), mksn(dstpath,last(ssn)))

	else:
		if no(dsn):
			# two snapshots on the sender, none here
			print "	s>1r0: sending latest snapshot in full"
			if not opts.dryrun:
				src_conn.transfer(dst_conn, mksn(srcpath,last(ssn)), mksn(dstpath,last(ssn)))
		else:
			if last(ssn) == last(dsn):
				# latest matching snapshots on both sides
				print "	s>1rXlatestmatch: Latest snapshots match on both sides, skipping"
			elif last(dsn) in ssn:
				# two snapshots on the sender, the old one matching here
				print "	s>1rXmatch: sending incremental snapshot"
				if not opts.dryrun:
					src_conn.transfer(dst_conn, mksn(srcpath,last(ssn)), mksn(dstpath,last(ssn)), last(dsn))
			else:
				print "	s2r1nomatch: destroying receiver side filesystems, sending full snapshot"
				if not opts.dryrun:
					dst_conn.destroy(dstpath)
					src_conn.transfer(dst_conn, mksn(srcpath,last(ssn)), mksn(dstpath,last(ssn)))

	backed_up_datasets.add(dstpath)
	print ""

print "Backup finished."

print ""

# THE PRUNE DELETED DATASETS ALGORITHM
# reinspect pools
src_conn.pools
dst_conn.pools

print "Removing from destination datasets not present on source"

all_datasets_in_destination = sets.Set([ x.get_path() for x in destination_dataset.walk() if not isinstance(x,Snapshot) ])

for dataset in children_first(all_datasets_in_destination - backed_up_datasets):
	print "	Dataset %s was not present in source during backup, removing"%dataset
	if not opts.dryrun:
		dst_conn.destroy(dataset)

print "Obsolete dataset removal finished"

print ""


# THE REMOVE OBSOLETE SNAPSHOTS ALGORITHM
# reinspect pools
src_conn.pools
dst_conn.pools

print "Removing non-common and old snapshots"

all_snapshots = sets.Set([ x.name for x in
		list(source_dataset.walk()) + list(destination_dataset.walk())
		if isinstance(x,Snapshot) and flt(x) ])
all_datasets = [ x for x in
		list(source_dataset.walk()) + list(destination_dataset.walk())
		if not isinstance(x,Snapshot) ]
noncommon_snapshots = sets.Set()
common_snapshots = sets.Set()

for snap in all_snapshots:
	in_all = True
	for dset in all_datasets:
		dsetsnaps = [ c for c in dset.children if c.name == snap ]
		if not dsetsnaps: in_all = False
	if in_all: common_snapshots.add(snap)
	else: noncommon_snapshots.add(snap)

print "	Common snapshots:",common_snapshots
print "	Noncommon snapshots:",noncommon_snapshots
# latest one will remain
toremove = sets.Set(chronosorted(list(common_snapshots))[:-1] + list(noncommon_snapshots))
print "	Snapshots to remove:",toremove

for conn,dsets in ( (src_conn,source_dataset), (dst_conn,destination_dataset) ):
	for dset in reversed(list(dsets.walk())):
		if isinstance(dset,Snapshot) and dset.name in toremove:
			print "	Destroying %s"%dset
			if not opts.dryrun:
				conn.destroy(dset.get_path())

print "Snapshot removal finished"


src_conn.pools
dst_conn.pools
