#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import optparse
import os
import time
import sets
sys.path.append(os.path.dirname(os.path.realpath(__file__)))
from zfslib import children_first, parents_first, chronosorted
from zfslib import Dataset, Pool, Snapshot, PoolSet, ZFSConnection
from zfslib import stderr

#===================== configuration =====================

parser = optparse.OptionParser("usage: %prog [-onv] [-b BUFSIZE] [-l RATELIMIT] <srcdatasetname> <dstdatasetname>")
parser.add_option('-o', '--progress', action='store_true', dest='progress', default=False, help='show progress (depends on the executabilty of the \'bar\' program) (default: %default)')
parser.add_option('-l', '--rate-limit', action='store', dest='ratelimit', default=-1, type="int", help='rate limit in bytes per second (requires --progress) (default: %default which means no limit)')
parser.add_option('-n', '--dry-run', action='store_true', dest='dryrun', default=False, help='don\'t actually manipulate any file systems')
parser.add_option('-b', '--bufsize', action='store', dest='bufsize', default=1048576, help='buffer size in bytes for network operations (default: %default)')
parser.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='be verbose (default: %default)')
opts,args = parser.parse_args(sys.argv[1:])

try:
	bufsize = int(opts.bufsize)
	assert bufsize >= 16384
except (ValueError,AssertionError),e:
	parser.error("error: bufsize must be an integer greater than 16384")

if len(args) == 2:
	try: source_host, source_dataset_name = args[0].split(":",1)
	except ValueError: source_host, source_dataset_name = "localhost",args[0]
	try: destination_host, destination_dataset_name = args[1].split(":",1)
	except ValueError: destination_host, destination_dataset_name = "localhost",args[1]
else:
	parser.error("error: arguments are wrong")

if opts.ratelimit != -1:
    if opts.ratelimit < 1024:
        parser.error("error: rate limit (%s) must be higher than 1024 bytes per second"%opts.ratelimit)
    if not opts.progress:
        parser.error("error: to apply a rate limit, you must specify --progress too")

def verbose_stderr(*args,**kwargs):
	if opts.verbose: stderr(*args,**kwargs)

#===================== end configuration =================

# ================ start program algorithm ===================

src_conn = ZFSConnection(source_host)
dst_conn = ZFSConnection(destination_host)

verbose_stderr("Replicating dataset %s:%s into %s:%s" % (
		source_host,source_dataset_name,
		destination_host,destination_dataset_name))

verbose_stderr("Assessing that the source dataset exists...")
try:
	source_dataset = src_conn.pools.lookup(source_dataset_name)
	verbose_stderr("%s: OK" % source_dataset)
except KeyError:
	stderr("Error: the source dataset does not exist.  Backup cannot continue.")
	sys.exit(2)


verbose_stderr("Assessing that the destination dataset exists...")
try:
	destination_dataset = dst_conn.pools.lookup(destination_dataset_name)
	verbose_stderr("%s: OK" % destination_dataset)
except KeyError:
	stderr("Error: the destination dataset does not exist.  Backup cannot continue.")
	sys.exit(2)


# it may be worthwhile to check for the invariant that the latest source snapshot should be
# present in EVERY SINGLE ONE of the children of the dataset
# then again it may not, because there has to be at least ONE snapshot for this command to work
common = sets.Set( [ x.name for x in source_dataset.get_snapshots(lambda _: True) ] )
common = [ source_dataset.get_child(x) for x in common ]
common = [ (x.get_creation(),x)        for x in common ]
common = chronosorted(common)
target_snapshot = common[-1][1].get_path()
target_snapshot_name = common[-1][1].name

#for x in source_dataset.walk():
#	if isinstance(x,Snapshot): continue
#	x.get_child(target_snapshot_name)

common = sets.Set( [ x.name for x in destination_dataset.get_snapshots(lambda _: True) ] )
common = common.intersection( sets.Set( [ x.name for x in source_dataset.get_snapshots(lambda _: True) ] ) )
common = [ destination_dataset.get_child(x) for x in common ]
common = [ (x.get_creation(), x)            for x in common ]
common = chronosorted(common)
try:
	base_snapshot =      common[-1][1].get_path()
	base_snapshot_name = common[-1][1].name
	full = False
except IndexError:
	full = True


send_opts = ["-R"]
receive_opts = []
if opts.dryrun:
	receive_opts.append("-n")
if opts.verbose:
	send_opts.append("-v")
	receive_opts.append("-v")

if full:
	verbose_stderr("No matching destination snapshots, transferring full replication stream")

	src_conn.transfer(dst_conn, target_snapshot,destination_dataset_name,showprogress=opts.progress,ratelimit=opts.ratelimit,bufsize=bufsize,send_opts=send_opts,receive_opts=receive_opts)
	
elif target_snapshot_name == base_snapshot_name:
	verbose_stderr("Base snapshot to reference in destination: %s" % base_snapshot_name)
	verbose_stderr("Target snapshot available in source: %s" % target_snapshot_name)
	verbose_stderr("Snapshots are up to date, exiting")
	
else: #incremental
	verbose_stderr("Base snapshot to reference in destination: %s" % base_snapshot_name)
	verbose_stderr("Target snapshot available in source: %s" % target_snapshot_name)
	verbose_stderr("Transferring differential replication stream")

	send_opts.append("-I")
	send_opts.append(base_snapshot_name)
	
	src_conn.transfer(dst_conn, target_snapshot,destination_dataset_name,showprogress=opts.progress,ratelimit=opts.ratelimit,bufsize=bufsize,send_opts=send_opts,receive_opts=receive_opts)

src_conn.pools
dst_conn.pools
