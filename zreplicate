#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import optparse
import os
import time
import sets
sys.path.append(os.path.dirname(os.path.realpath(__file__)))
from zfslib import children_first, parents_first, chronosorted
from zfslib import Dataset, Pool, Snapshot, PoolSet, ZFSConnection

#===================== configuration =====================

parser = optparse.OptionParser("usage: %prog [-on] [-b BUFSIZE] <srcdatasetname> <dstdatasetname>")
parser.add_option('-o', '--progress', action='store_true', dest='progress', default=False, help='show progress (depends on the executabilty of the \'bar\' program) (default: %default)')
parser.add_option('-n', '--dry-run', action='store_true', dest='dryrun', default=False, help='don\'t actually manipulate any file systems')
parser.add_option('-b', '--bufsize', action='store', dest='bufsize', default=1048576, help='buffer size in bytes for network operations (default: %default)')
opts,args = parser.parse_args(sys.argv[1:])

try:
	bufsize = int(opts.bufsize)
	assert bufsize >= 16384
except (ValueError,AssertionError),e:
	sys.stderr.write("error: bufsize must be an integer greater than 16384\n%s"%parser.get_usage())
	sys.exit(os.EX_USAGE)

if len(args) == 2:
	try: source_host, source_dataset_name = args[0].split(":",1)
	except ValueError: source_host, source_dataset_name = "localhost",args[0]
	try: destination_host, destination_dataset_name = args[1].split(":",1)
	except ValueError: destination_host, destination_dataset_name = "localhost",args[1]
else:
	sys.stderr.write("error: arguments are wrong\n%s"%parser.get_usage())
	sys.exit(os.EX_USAGE)

#===================== end configuration =================

# ================ start program algorithm ===================

src_conn = ZFSConnection(source_host)
dst_conn = ZFSConnection(destination_host)

print "Replicating dataset %s:%s into %s:%s"%(source_host,source_dataset_name,
	destination_host,destination_dataset_name)

print ""

print "Assessing that the source dataset exists...",
try:
	source_dataset = src_conn.pools.lookup(source_dataset_name)
	print "%s: OK"%source_dataset
except KeyError:
	print "No.\nError: the source dataset does not exist.  Backup cannot continue"
	sys.exit(2)

print ""


try: destination_dataset = dst_conn.pools.lookup(destination_dataset_name)
except KeyError: destination_dataset = None
print "Does the destination dataset exist on the destination side? ",

if destination_dataset:
	print "Yes."
else:
	print "No.\nError: the destination dataset does not exist.  Backup cannot continue"
	sys.exit(2)

print ""


ssn = chronosorted([ (x.name,x) for x in source_dataset.get_snapshots(lambda _: True) ]) # Source 
latest_source_snapshot_name = ssn[-1][0]
latest_source_snapshot = ssn[-1][1].get_path()

dsn = chronosorted([ (x.name,x) for x in destination_dataset.get_snapshots(lambda _: True) ]) # Destination
try:
	common = sets.Set([ x[0] for x in ssn])
	common = common.intersection(sets.Set([ x[0] for x in dsn]))
	common = chronosorted(common)
	latest_destination_snapshot_name = common[-1]
	latest_destination_snapshot = dict(dsn)[latest_destination_snapshot_name]
	full = False
except IndexError:
	full = True

send_opts = ["-R"]
receive_opts = []
if opts.dryrun: receive_opts.append("-n")

if full:
	src_conn.transfer(dst_conn, latest_source_snapshot,destination_dataset_name,showprogress=opts.progress,bufsize=bufsize,send_opts=send_opts,receive_opts=receive_opts)
	
elif latest_destination_snapshot_name == latest_source_snapshot_name:
	print "Snapshots are up to date, exiting"
	
else: #incremental
	send_opts.append("-I")
	send_opts.append(latest_destination_snapshot_name)
	
	src_conn.transfer(dst_conn, latest_source_snapshot,destination_dataset_name,showprogress=opts.progress,bufsize=bufsize,send_opts=send_opts,receive_opts=receive_opts)

src_conn.pools
dst_conn.pools
